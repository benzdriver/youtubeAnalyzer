# Task 10: éƒ¨ç½²é…ç½®

## ä»»åŠ¡æ¦‚è¿°

é…ç½®YouTubeåˆ†æå·¥å…·çš„ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼ŒåŒ…æ‹¬Dockerå®¹å™¨åŒ–ã€CI/CDæµæ°´çº¿ã€ç¯å¢ƒé…ç½®ã€ç›‘æ§å‘Šè­¦å’Œæ–‡æ¡£å®Œå–„ã€‚ç¡®ä¿ç³»ç»Ÿèƒ½å¤Ÿç¨³å®šã€å®‰å…¨åœ°åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡Œã€‚

## ç›®æ ‡

- é…ç½®ç”Ÿäº§ç¯å¢ƒçš„Dockeréƒ¨ç½²
- å»ºç«‹CI/CDè‡ªåŠ¨åŒ–æµæ°´çº¿
- å®ç°ç¯å¢ƒé…ç½®å’Œå¯†é’¥ç®¡ç†
- é…ç½®ç›‘æ§ã€æ—¥å¿—å’Œå‘Šè­¦ç³»ç»Ÿ
- ç¼–å†™éƒ¨ç½²å’Œè¿ç»´æ–‡æ¡£

## å¯äº¤ä»˜æˆæœ

### 1. ç”Ÿäº§ç¯å¢ƒDockeré…ç½®

#### Docker Composeç”Ÿäº§é…ç½®
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - static_files:/var/www/static
    depends_on:
      - backend
      - frontend
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: production
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=https://api.youtubeanalyzer.com
      - NEXT_PUBLIC_WS_URL=wss://api.youtubeanalyzer.com
    restart: unless-stopped

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    environment:
      - ENVIRONMENT=production
      - DEBUG=false
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - YOUTUBE_API_KEY=${YOUTUBE_API_KEY}
      - SECRET_KEY=${SECRET_KEY}
    volumes:
      - storage_data:/app/storage
      - logs:/app/logs
    depends_on:
      - postgres
      - redis
    restart: unless-stopped

  worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    command: celery -A app.main worker --loglevel=info --concurrency=4
    environment:
      - ENVIRONMENT=production
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - YOUTUBE_API_KEY=${YOUTUBE_API_KEY}
    volumes:
      - storage_data:/app/storage
      - logs:/app/logs
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    deploy:
      replicas: 2

  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  storage_data:
  static_files:
  logs:

networks:
  default:
    driver: bridge
```

#### ç”Ÿäº§ç¯å¢ƒDockerfileä¼˜åŒ–
```dockerfile
# backend/Dockerfile
FROM python:3.11-slim as base

RUN apt-get update && apt-get install -y \
    ffmpeg \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

FROM base as development
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

FROM base as production
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

RUN useradd --create-home --shell /bin/bash app \
    && mkdir -p /app/storage /app/logs \
    && chown -R app:app /app

COPY . .
RUN chown -R app:app /app

USER app

HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

EXPOSE 8000

CMD ["gunicorn", "app.main:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000"]
```

### 2. CI/CDæµæ°´çº¿é…ç½®

#### GitHub Actionså·¥ä½œæµ
```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install backend dependencies
      run: |
        cd backend
        pip install -r requirements.txt
    
    - name: Run backend tests
      run: |
        cd backend
        pytest --cov=app --cov-report=xml
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost/test_db
        REDIS_URL: redis://localhost:6379
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Run frontend tests
      run: |
        cd frontend
        npm run test
        npm run build

  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to production
      uses: appleboy/ssh-action@v1.0.0
      with:
        host: ${{ secrets.PROD_HOST }}
        username: ${{ secrets.PROD_USER }}
        key: ${{ secrets.PROD_SSH_KEY }}
        script: |
          cd /opt/youtube-analyzer
          git pull origin main
          docker-compose -f docker-compose.prod.yml pull
          docker-compose -f docker-compose.prod.yml up -d
          docker system prune -f
    
    - name: Health check
      run: |
        sleep 30
        curl -f ${{ secrets.PROD_URL }}/health || exit 1
```

### 3. ç¯å¢ƒé…ç½®å’Œå¯†é’¥ç®¡ç†

#### ç¯å¢ƒå˜é‡æ¨¡æ¿
```bash
# .env.prod.example
ENVIRONMENT=production
DEBUG=false
SECRET_KEY=your-super-secret-key-here
ALLOWED_ORIGINS=https://youtubeanalyzer.com

DATABASE_URL=postgresql://username:password@postgres:5432/youtube_analyzer
POSTGRES_DB=youtube_analyzer
POSTGRES_USER=youtube_user
POSTGRES_PASSWORD=secure-password

REDIS_URL=redis://:password@redis:6379/0
REDIS_PASSWORD=redis-password

OPENAI_API_KEY=sk-your-openai-api-key
YOUTUBE_API_KEY=your-youtube-api-key

SSL_CERT_PATH=/etc/nginx/ssl/cert.pem
SSL_KEY_PATH=/etc/nginx/ssl/key.pem
```

#### Nginxé…ç½®
```nginx
# nginx/nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream backend {
        server backend:8000;
    }
    
    upstream frontend {
        server frontend:3000;
    }

    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;

    server {
        listen 80;
        server_name youtubeanalyzer.com;
        return 301 https://$server_name$request_uri;
    }

    server {
        listen 443 ssl http2;
        server_name youtubeanalyzer.com;

        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;

        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        add_header Strict-Transport-Security "max-age=63072000; includeSubDomains; preload";

        location / {
            proxy_pass http://frontend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        location /api/ {
            limit_req zone=api burst=20 nodelay;
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        location /ws/ {
            proxy_pass http://backend;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
        }

        location /static/ {
            alias /var/www/static/;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
    }
}
```

### 4. ç›‘æ§å’Œæ—¥å¿—é…ç½®

#### åº”ç”¨ç›‘æ§é…ç½®
```python
# backend/app/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import time
import logging

# å®šä¹‰ç›‘æ§æŒ‡æ ‡
REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('http_request_duration_seconds', 'HTTP request duration')
ACTIVE_TASKS = Gauge('active_analysis_tasks', 'Number of active analysis tasks')
TASK_COMPLETION_TIME = Histogram('task_completion_seconds', 'Task completion time', ['task_type'])

class MonitoringMiddleware:
    def __init__(self, app):
        self.app = app
    
    async def __call__(self, scope, receive, send):
        if scope["type"] == "http":
            start_time = time.time()
            
            # åŒ…è£…sendå‡½æ•°æ¥æ•è·å“åº”çŠ¶æ€
            status_code = 200
            async def wrapped_send(message):
                nonlocal status_code
                if message["type"] == "http.response.start":
                    status_code = message["status"]
                await send(message)
            
            await self.app(scope, receive, wrapped_send)
            
            # è®°å½•æŒ‡æ ‡
            duration = time.time() - start_time
            method = scope["method"]
            path = scope["path"]
            
            REQUEST_COUNT.labels(method=method, endpoint=path, status=status_code).inc()
            REQUEST_DURATION.observe(duration)
        else:
            await self.app(scope, receive, send)

def get_metrics():
    """è¿”å›Prometheusæ ¼å¼çš„æŒ‡æ ‡"""
    return generate_latest()
```

#### æ—¥å¿—é…ç½®
```python
# backend/app/core/logging_config.py
import logging
import logging.config
from pathlib import Path

LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'detailed': {
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        },
        'json': {
            'format': '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "logger": "%(name)s", "message": "%(message)s"}'
        }
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'level': 'INFO',
            'formatter': 'detailed',
            'stream': 'ext://sys.stdout'
        },
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'level': 'DEBUG',
            'formatter': 'json',
            'filename': '/app/logs/app.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5
        },
        'error_file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'level': 'ERROR',
            'formatter': 'json',
            'filename': '/app/logs/error.log',
            'maxBytes': 10485760,
            'backupCount': 5
        }
    },
    'loggers': {
        'app': {
            'level': 'DEBUG',
            'handlers': ['console', 'file', 'error_file'],
            'propagate': False
        },
        'uvicorn': {
            'level': 'INFO',
            'handlers': ['console', 'file'],
            'propagate': False
        }
    },
    'root': {
        'level': 'INFO',
        'handlers': ['console']
    }
}

def setup_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    Path('/app/logs').mkdir(exist_ok=True)
    
    logging.config.dictConfig(LOGGING_CONFIG)
```

### 5. éƒ¨ç½²è„šæœ¬å’Œæ–‡æ¡£

#### éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# scripts/deploy.sh

set -e

echo "ğŸš€ å¼€å§‹éƒ¨ç½²YouTubeåˆ†æå·¥å…·..."

# æ£€æŸ¥ç¯å¢ƒå˜é‡
if [ ! -f .env.prod ]; then
    echo "âŒ é”™è¯¯: .env.prod æ–‡ä»¶ä¸å­˜åœ¨"
    exit 1
fi

# å¤‡ä»½å½“å‰ç‰ˆæœ¬
echo "ğŸ“¦ å¤‡ä»½å½“å‰ç‰ˆæœ¬..."
docker-compose -f docker-compose.prod.yml down
docker tag youtubeanalyzer_backend:latest youtubeanalyzer_backend:backup-$(date +%Y%m%d-%H%M%S) || true
docker tag youtubeanalyzer_frontend:latest youtubeanalyzer_frontend:backup-$(date +%Y%m%d-%H%M%S) || true

# æ‹‰å–æœ€æ–°ä»£ç 
echo "ğŸ“¥ æ‹‰å–æœ€æ–°ä»£ç ..."
git pull origin main

# æ„å»ºæ–°é•œåƒ
echo "ğŸ”¨ æ„å»ºDockeré•œåƒ..."
docker-compose -f docker-compose.prod.yml build

# æ•°æ®åº“è¿ç§»
echo "ğŸ—„ï¸ æ‰§è¡Œæ•°æ®åº“è¿ç§»..."
docker-compose -f docker-compose.prod.yml run --rm backend alembic upgrade head

# å¯åŠ¨æœåŠ¡
echo "ğŸš€ å¯åŠ¨æœåŠ¡..."
docker-compose -f docker-compose.prod.yml up -d

# å¥åº·æ£€æŸ¥
echo "ğŸ” æ‰§è¡Œå¥åº·æ£€æŸ¥..."
sleep 30

if curl -f http://localhost/health; then
    echo "âœ… éƒ¨ç½²æˆåŠŸï¼"
    
    # æ¸…ç†æ—§é•œåƒ
    echo "ğŸ§¹ æ¸…ç†æ—§é•œåƒ..."
    docker system prune -f
else
    echo "âŒ å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œå›æ»šåˆ°å¤‡ä»½ç‰ˆæœ¬..."
    docker-compose -f docker-compose.prod.yml down
    
    # å›æ»šé€»è¾‘
    BACKUP_BACKEND=$(docker images --format "table {{.Repository}}:{{.Tag}}" | grep youtubeanalyzer_backend:backup | head -1)
    BACKUP_FRONTEND=$(docker images --format "table {{.Repository}}:{{.Tag}}" | grep youtubeanalyzer_frontend:backup | head -1)
    
    if [ ! -z "$BACKUP_BACKEND" ] && [ ! -z "$BACKUP_FRONTEND" ]; then
        docker tag $BACKUP_BACKEND youtubeanalyzer_backend:latest
        docker tag $BACKUP_FRONTEND youtubeanalyzer_frontend:latest
        docker-compose -f docker-compose.prod.yml up -d
        echo "ğŸ”„ å·²å›æ»šåˆ°å¤‡ä»½ç‰ˆæœ¬"
    fi
    
    exit 1
fi

echo "ğŸ‰ éƒ¨ç½²å®Œæˆï¼"
```

#### è¿ç»´æ–‡æ¡£
```markdown
# YouTubeåˆ†æå·¥å…·è¿ç»´æ‰‹å†Œ

## æœåŠ¡ç®¡ç†

### å¯åŠ¨æœåŠ¡
```bash
docker-compose -f docker-compose.prod.yml up -d
```

### åœæ­¢æœåŠ¡
```bash
docker-compose -f docker-compose.prod.yml down
```

### æŸ¥çœ‹æ—¥å¿—
```bash
# æŸ¥çœ‹æ‰€æœ‰æœåŠ¡æ—¥å¿—
docker-compose -f docker-compose.prod.yml logs -f

# æŸ¥çœ‹ç‰¹å®šæœåŠ¡æ—¥å¿—
docker-compose -f docker-compose.prod.yml logs -f backend
docker-compose -f docker-compose.prod.yml logs -f worker
```

### æ‰©å®¹Worker
```bash
docker-compose -f docker-compose.prod.yml up -d --scale worker=4
```

## ç›‘æ§å’Œå‘Šè­¦

### å…³é”®æŒ‡æ ‡
- HTTPè¯·æ±‚å“åº”æ—¶é—´
- é”™è¯¯ç‡
- æ´»è·ƒä»»åŠ¡æ•°é‡
- æ•°æ®åº“è¿æ¥æ•°
- å†…å­˜å’ŒCPUä½¿ç”¨ç‡

### å‘Šè­¦é˜ˆå€¼
- å“åº”æ—¶é—´ > 2ç§’
- é”™è¯¯ç‡ > 5%
- å†…å­˜ä½¿ç”¨ç‡ > 90%
- ç£ç›˜ä½¿ç”¨ç‡ > 85%

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

1. **æœåŠ¡æ— æ³•å¯åŠ¨**
   - æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®
   - æŸ¥çœ‹Dockeræ—¥å¿—
   - éªŒè¯ç«¯å£å ç”¨æƒ…å†µ

2. **åˆ†æä»»åŠ¡å¤±è´¥**
   - æ£€æŸ¥APIå¯†é’¥é…ç½®
   - æŸ¥çœ‹Workeræ—¥å¿—
   - éªŒè¯ç½‘ç»œè¿æ¥

3. **æ•°æ®åº“è¿æ¥é—®é¢˜**
   - æ£€æŸ¥æ•°æ®åº“æœåŠ¡çŠ¶æ€
   - éªŒè¯è¿æ¥å­—ç¬¦ä¸²
   - æŸ¥çœ‹è¿æ¥æ± çŠ¶æ€

### æ€§èƒ½ä¼˜åŒ–

1. **æ•°æ®åº“ä¼˜åŒ–**
   - å®šæœŸæ‰§è¡ŒVACUUM
   - ç›‘æ§æ…¢æŸ¥è¯¢
   - ä¼˜åŒ–ç´¢å¼•

2. **ç¼“å­˜ä¼˜åŒ–**
   - ç›‘æ§Rediså†…å­˜ä½¿ç”¨
   - è°ƒæ•´ç¼“å­˜è¿‡æœŸæ—¶é—´
   - ä¼˜åŒ–ç¼“å­˜é”®è®¾è®¡

## å¤‡ä»½å’Œæ¢å¤

### æ•°æ®åº“å¤‡ä»½
```bash
# åˆ›å»ºå¤‡ä»½
docker exec postgres pg_dump -U youtube_user youtube_analyzer > backup_$(date +%Y%m%d).sql

# æ¢å¤å¤‡ä»½
docker exec -i postgres psql -U youtube_user youtube_analyzer < backup_20240101.sql
```

### æ–‡ä»¶å¤‡ä»½
```bash
# å¤‡ä»½å­˜å‚¨æ–‡ä»¶
tar -czf storage_backup_$(date +%Y%m%d).tar.gz /opt/youtube-analyzer/storage
```
```

## ä¾èµ–å…³ç³»

### å‰ç½®æ¡ä»¶
- Task 1: é¡¹ç›®é…ç½®ç®¡ç†ï¼ˆå¿…é¡»å®Œæˆï¼‰
- Task 2: åç«¯APIæ¡†æ¶ï¼ˆå¿…é¡»å®Œæˆï¼‰
- Task 3: å‰ç«¯UIæ¡†æ¶ï¼ˆå¿…é¡»å®Œæˆï¼‰
- Task 4-9: æ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼ˆå¿…é¡»å®Œæˆï¼‰

### é˜»å¡ä»»åŠ¡
- æ— ï¼ˆè¿™æ˜¯æœ€åçš„éƒ¨ç½²ä»»åŠ¡ï¼‰

## éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶
- [ ] Dockerå®¹å™¨åŒ–é…ç½®æ­£ç¡®
- [ ] CI/CDæµæ°´çº¿æ­£å¸¸å·¥ä½œ
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æˆåŠŸ
- [ ] ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿæ­£å¸¸
- [ ] æ—¥å¿—æ”¶é›†å’Œåˆ†ææ­£å¸¸
- [ ] å¤‡ä»½å’Œæ¢å¤æµç¨‹æ­£å¸¸

### æŠ€æœ¯éªŒæ”¶
- [ ] æœåŠ¡å¯åŠ¨æ—¶é—´ < 2åˆ†é’Ÿ
- [ ] ç³»ç»Ÿå¯ç”¨æ€§ â‰¥ 99.5%
- [ ] è‡ªåŠ¨æ‰©å®¹æœºåˆ¶æ­£å¸¸
- [ ] å®‰å…¨é…ç½®ç¬¦åˆæ ‡å‡†
- [ ] æ€§èƒ½ç›‘æ§è¦†ç›–å…¨é¢

### è´¨é‡éªŒæ”¶
- [ ] éƒ¨ç½²æ–‡æ¡£å®Œæ•´å‡†ç¡®
- [ ] è¿ç»´æ‰‹å†Œè¯¦ç»†å®ç”¨
- [ ] æ•…éšœæ’æŸ¥æŒ‡å—æœ‰æ•ˆ
- [ ] å®‰å…¨å®¡è®¡é€šè¿‡
- [ ] æ€§èƒ½æµ‹è¯•é€šè¿‡

## æµ‹è¯•è¦æ±‚

### éƒ¨ç½²æµ‹è¯•
```bash
# æµ‹è¯•è„šæœ¬
#!/bin/bash
# tests/deployment_test.sh

echo "ğŸ§ª å¼€å§‹éƒ¨ç½²æµ‹è¯•..."

# æµ‹è¯•Dockeræ„å»º
docker-compose -f docker-compose.prod.yml build
if [ $? -ne 0 ]; then
    echo "âŒ Dockeræ„å»ºå¤±è´¥"
    exit 1
fi

# æµ‹è¯•æœåŠ¡å¯åŠ¨
docker-compose -f docker-compose.prod.yml up -d
sleep 60

# å¥åº·æ£€æŸ¥
if curl -f http://localhost/health; then
    echo "âœ… å¥åº·æ£€æŸ¥é€šè¿‡"
else
    echo "âŒ å¥åº·æ£€æŸ¥å¤±è´¥"
    exit 1
fi

# åŠŸèƒ½æµ‹è¯•
python tests/integration_test.py

# æ¸…ç†
docker-compose -f docker-compose.prod.yml down

echo "âœ… éƒ¨ç½²æµ‹è¯•å®Œæˆ"
```

## é¢„ä¼°å·¥ä½œé‡

- **Dockeré…ç½®**: 1å¤©
- **CI/CDé…ç½®**: 1.5å¤©
- **ç›‘æ§é…ç½®**: 1å¤©
- **éƒ¨ç½²è„šæœ¬**: 0.5å¤©
- **æ–‡æ¡£ç¼–å†™**: 1å¤©
- **æµ‹è¯•éªŒè¯**: 1å¤©
- **æ€»è®¡**: 6å¤©

## å…³é”®è·¯å¾„

æ­¤ä»»åŠ¡æ˜¯æ•´ä¸ªé¡¹ç›®çš„æœ€åä¸€æ­¥ï¼Œå®Œæˆåç³»ç»Ÿå³å¯æŠ•å…¥ç”Ÿäº§ä½¿ç”¨ã€‚

## äº¤ä»˜æ£€æŸ¥æ¸…å•

- [ ] Dockerç”Ÿäº§é…ç½®å·²å®Œæˆ
- [ ] CI/CDæµæ°´çº¿å·²é…ç½®
- [ ] ç¯å¢ƒå˜é‡ç®¡ç†å·²å®ç°
- [ ] Nginxåå‘ä»£ç†å·²é…ç½®
- [ ] ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿå·²éƒ¨ç½²
- [ ] éƒ¨ç½²è„šæœ¬å·²ç¼–å†™
- [ ] è¿ç»´æ–‡æ¡£å·²å®Œæˆ
- [ ] å®‰å…¨é…ç½®å·²å®æ–½
- [ ] å¤‡ä»½ç­–ç•¥å·²åˆ¶å®š
- [ ] éƒ¨ç½²æµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æµ‹è¯•é€šè¿‡
- [ ] å®‰å…¨å®¡è®¡é€šè¿‡

## åç»­ç»´æŠ¤

å®Œæˆæ­¤ä»»åŠ¡åï¼Œç³»ç»Ÿå°†å…·å¤‡ï¼š
- è‡ªåŠ¨åŒ–éƒ¨ç½²èƒ½åŠ›
- å®Œæ•´çš„ç›‘æ§ä½“ç³»
- æ ‡å‡†åŒ–çš„è¿ç»´æµç¨‹
- å¯é çš„å¤‡ä»½æ¢å¤æœºåˆ¶
- è¯¦ç»†çš„æ•…éšœæ’æŸ¥æŒ‡å—

è¿™ä¸ºYouTubeåˆ†æå·¥å…·çš„é•¿æœŸç¨³å®šè¿è¡Œæä¾›äº†åšå®çš„åŸºç¡€ã€‚
